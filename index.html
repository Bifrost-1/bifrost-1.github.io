<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents">
  <meta name="keywords" content="image generation,image understanding,image understanding and generation,MLLM,unified model,bifrost-1,bifrost,bifrost1">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Bifrost-1</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://hl-hanlin.github.io/">Han Lin<sup>1</sup></a>
            </span>, &nbsp;
            <span class="author-block">
              <a href="https://j-min.io">Jaemin Cho<sup>1</sup></a>
            </span>, &nbsp;
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=MQFngiMAAAAJ">Amir Zadeh<sup>2</sup></a>
            </span>, &nbsp;
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=hoZesOwAAAAJ">Chuan Li<sup>2</sup></a>
            </span>, &nbsp;
              <span class="author-block">
                <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal<sup>1</sup></a>
              </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">UNC Chapel Hill<sup>1</sup></span>
             &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
            <span class="author-block">Lambda<sup>2</sup></span>
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block" style="font-weight: bold; font-size: 1.3em;">COLM 2024</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/HL-hanlin/Bifrost-1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <!-- Model Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/hanlincs/Bifrost-1/tree/main"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img style="max-width: 16px;" src="https://ctrl-adapter.github.io/static/hf_fa.ico" alt="Hugging Face">
                  </span>
                  <span>Model</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <img src="./static/images/teaser.png" alt="Teaser" width="90%">
        <div class="content has-text-justified">
          <b>Figure 1. Overview of Bifrost-1.</b>
        Bifrost-1 equips the backbone MLLM with a visual generation branch, which is a trainable copy of a pretrained MLLM parameters (i.e., QKV, MLP, normalization layers) and a newly added vision head (i.e., a linear layer).
        The visual generation branch outputs patch-level CLIP latents, which are then downsampled and reshaped into 2D (HxW), provided to latent ControlNet, and finally guiding image generation of a pretrained diffusion model.
        During training, a portion of the image patches is randomly replaced with learnable mask tokens &lt;M&gt;.
        During inference, we start with fully masked image tokens and autoregressively predict them.
        </div>

      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          There is growing interest in integrating high-fidelity visual synthesis capabilities into large language models (LLMs) without compromising their strong reasoning capabilities.
          Existing methods that directly train LLMs or bridge LLMs and diffusion models usually suffer from costly training since the backbone LLMs have not seen image representations during pretraining.
          We present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs (MLLMs) and diffusion models using patch-level CLIP image embeddings as latent variables, which are natively aligned with the MLLM's CLIP visual encoder.
          These patch-level image embeddings are integrated into the diffusion model with a lightweight adaptation of its ControlNet.
          To retain the original multimodal reasoning capabilities of MLLMs, we equip the MLLM with a visual generation branch initialized from the original MLLM parameters when predicting the patch-level image embeddings.
          By seamlessly integrating pretrained MLLMs and diffusion models
          with patch-level CLIP latents, our framework enables high-fidelity controllable image generation with significant training efficiency.
          Our experiments demonstrate that Bifrost-1 achieves comparable or better performance than previous methods in terms of visual fidelity and multimodal understanding, with substantially lower compute during training.
          We also provide comprehensive ablation studies showing the effectiveness of our design choices.
          </p>

        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model Architecture</h2>

        <div class="content has-text-justified">
          <p>
          <b> Bifrost-1 design summary.</b>
          We have two main goals:
          (1) to preserve the multimodal understanding capability of the MLLM,
          while (2) efficiently teaching MLLM to generate latent tokens to guide diffusion models.
          As illustrated in Figure 1, Bifrost-1 achieves these goals by having a new visual generation branch initialized from the original MLLM parameters and using patch-level CLIP image embeddings as latent bridge tokens.
          </p>
          <p>
          <b> Learning to unmask image patch embeddings with a visual generation branch.</b>
          To teach an MLLM image generation, we first encode the images using the MLLM's native visual encoder to get patch-level image embeddings and concatenate them with text tokens.
          Following MAR, we replace parts of the input image embeddings with a learnable mask token &lt;M&gt; and let the MLLM to predict the masked image embeddings.
          For this image embedding prediction task, we introduce a visual generation branch (Figure 1 left), whose parameters are initialized from MLLM parameters (i.e., attention QKV projections, MLP projection layers, and normalization layers) following LMFusion.
          Just like text head, which is a linear layer toward text embedding space, we use a simple linear layer as a vision generation head.
          By reusing majority of parameters from the pretrained MLLM and randomly initializing only a single linear layer as the vision head, we avoid the costly process of realigning image embeddings.
          </p>
          <p>
          <b> Latent ControlNet.</b>
          To effectively guide diffusion models with patch-level CLIP image embeddings,
          we create latent ControlNet (Figure 1 top right),
          by modifying the original ControlNet architecture from a backbone image diffusion model (e.g., FLUX.1-dev) to take CLIP latents as input.
          During training, we update only the newly added input linear projection, the 2D downsampling convolution blocks, and the ControlNet for 4 MM-DiT blocks and 1 Single-DiT block, compared to the full FLUX.1-dev model which contains 19 MM-DiT blocks and 38 Single-DiT blocks in total.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Quantitative Results</h2>

        <br>

        <h3 class="title is-4">Comparison of Different Architectures</h3>
        <img src="./static/images/table1.png" alt="Teaser" width="90%">
        <div class="content has-text-justified">
          <b>Table 1. Comparison of different architectures for bridging LLMs and diffusion models in image generation on ImageNet 256x256.</b>
        </div>
        <div class="content has-text-justified">

        <p>
        In Table 1, we compare different choices for bridging LLMs and diffusion models in terms of image generation task on ImageNet 256x256.
        We generate 10K images with classes randomly sampled from 1k categories and compute visual quality metrics.
        In all settings, the MLLM visual generation branch and the Latent \cnet{} are trained for 16 and 2 epochs, respectively.
        </p>

        <p>
        <b>Bifrost-1 vs. Backbone diffusion model.</b>
        In the first and last rows of Table 1, we compare Bifrost-1 and its backbone diffusion model (FLUX.1-dev).
        We find that Bifrost-1 improves FID and sFID, while it hurts IS from the original backbone.
        The improvement over the baseline mainly comes from adding a few trainable ControlNet blocks to the backbone diffusion model, which enables better adaptation to the data distribution.
        </p>

        <p>
        <b>Patch-level CLIP latent vs. 2D learnable query tokens.</b>
        MetaQuery is a recent method that bridges LLMs and diffusion models by learning a connector (24-layer transformer encoder) that projects a frozen LLM's hidden representations on a finite number of learnable query tokens, where the connector outputs guide a diffusion model via cross attentions.
        Inspired by this, we implement a 2D version of MetaQuery and compare it with Bifrost-1.
        To fairly compare methods with similar additional parameters, instead of learning a heavy connector module, we directly reshape the LLM representations of learned query tokens as inputs to the latent ControlNet.
        As shown in Table 1, 2D learnable query tokens hurt performance in all 4 metrics, indicating that learning to align representations between MLLM and diffusion from scratch requires much more computation than our patch-level CLIP latent.
        </p>

        <p>
        <b>Patch-level CLIP latents vs. VAE latents.</b>
        We compare Bifrost-1 with a variant that replaces CLIP latents (that are natively aligned with MLLM) with VAE latents (that are not originally aligned with MLLMs). Specifically, we substitute the CLIP visual encoder with the FLUX VAE encoder, and replace our visual decoder (i.e., Latent ControlNet + FLUX diffusion model) with the FLUX VAE decoder. Linear projection layers are applied to align the dimensions of the VAE-encoded features with the feature dimension of the MLLM backbone. All other components of our framework and the training strategy are kept the same to ensure a fair comparison.
        As shown in the 3rd row of Table 1, using VAE features significantly slows down learning. Additionally, this highlights that by leveraging a pretrained diffusion model as the image renderer, Bifrost-1 effectively reduces the burden on the MLLM side to directly generate high-quality images.
        </p>

        <p>
        <b>MLLM's native visual encoder vs. non-aligned external visual encoder.</b>
        In addition, we also compare using the latents from MLLM's native visual encoder (i.e., CLIP) with an external visual encoder (i.e., SigLIP, as used in MetaMorph).
        As we can see in the 4th row of Table 1, although using SigLIP achieves better image generation quality than using VAE, it is still significantly worse than using the MLLM's native visual encoder. This indicates the training efficiency of adopting MLLM's native visual encoder compared with non-aligned external visual encoders.
        </p>

        <p>
        <b>Diffusion model guidance strategy: Latent ControlNet vs. Cross-attention.</b>
        Our method directly injects 2D image tokens generated by the MLLM into the DiT via latent ControlNet. By adding 2D ControlNet latents on top of the noisy DiT latents, our method can enforce spatial structures more explicitly and effectively.
        In contrast, several previous works use cross-attention to condition on image tokens.
        We conducted an additional experiment comparing these two conditioning strategies.
        For a fair comparison, we unfreeze all parameters in DiT when conditioning it via cross-attention and use 64 MLLM-generated image tokens for both methods.
        As shown in the 5th row of Table 1, our latent ControlNet achieves better image generation quality (FID, sFID, IS), demonstrating its superior efficiency and effectiveness.
        </p>

        </div>

        <br><br>
        <h3 class="title is-4">Image Generation</h3>
        <img src="./static/images/table2.png" alt="Teaser" width="90%">
        <div class="content has-text-justified">
          <b>Table 2. Comparison with state-of-the-art methods on multimodal generation benchmarks</b>
        </div>
        <div class="content has-text-justified">
          <p>
          In Table 2, we compare Bifrost-1 with the recent SoTA methods (e.g., JanusPro and MetaQuery) on MJHQ30k prompts.
          Table 2 compares Bifrost-1 with other unified models on image generation benchmarks, including COCO and MJHQ for visual quality and GenEval and DPG-Bench for prompt following ability.
          Trained only with 25M image-text pairs for less than two epochs, Bifrost-1 matches the performance with models trained with much higher compute, including Janus on GenEval benchmark, and outperforms TokenFlow-XL on DPG-Bench.
          In addition, we would like to highlight that FID scores heavily depend on the choice of diffusion backbones.
          As also observed in MetaQuery, diffusion models fine-tuned on aesthetic datasets (e.g., FLUX.1-dev) typically achieve worse FID scores compared to models that have not undergone extensive aesthetic fine-tuning (e.g., SD1.5, SANA).
          </p>
        </div>

        <br><br>
        <h3 class="title is-4">Image Understanding</h3>
        <img src="./static/images/table3.png" alt="Teaser" width="90%">
        <div class="content has-text-justified">
          <b>Table 3. Comparison with state-of-the-art methods on multimodal understanding benchmarks</b>
        </div>
        <div class="content has-text-justified">
          <p>
          Bifrost-1 fully inherits strong visual understanding capabilities of backbone MLLM.
          As Bifrost-1 keeps the original parameters in the MLLM completely frozen, the trainable vision branch is agnostic to the choice of MLLM.
          This allows Bifrost-1 to fully inherit the visual understanding capabilities of the MLLM and easily scale to backbone MLLMs with larger sizes and stronger performance.
          Table 3 compares the Qwen2.5-VL 7B model with other unified models on image understanding benchmarks, including MME-P, MMB, SEED, MMMU, and MM-Vet.
          Baseline models such as JanusPro and MetaMorph fully fine-tune the LLM backbone and therefore tend to lose the original planning and reasoning abilities that these LLMs acquired through large-scale text pretraining.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Examples & Analysis</h2>
        <br><br>
        <h3 class="title is-4">Image Reconstruction with Patch-level CLIP Latent</h3>

        <div class="content has-text-justified">
          In the following, we demonstrate the effectiveness of CLIP Latent-based image representation in image reconstruction experiments. Concretely, we experiment with scaling the number of latent tokens and compare Bifrost-1 with SoTA models.
        </div>


        <img src="./static/images/figure2.png" alt="Teaser" width="95%">
        <div class="content has-text-justified">
          <b>Figure 2. Image reconstruction scores with different numbers of 2D CLIP latent tokens</b>
        </div>

        <img src="./static/images/figure3.png" alt="Teaser" width="90%">
        <div class="content has-text-justified">
          <b>Figure 3. Visual samples for image reconstruction with different numbers of patch-level CLIP tokens generated from MLLM</b>
        </div>

        <div class="content has-text-justified">
          <p>
          In Figure 2, we show that image reconstruction quality scales well with the number of patch-level CLIP latent tokens input to the Latent ControlNet.
          With only 1 epoch of training on the ImageNet dataset, images represented by 256 (=14x14) CLIP latent tokens not only achieve higher reconstruction accuracy, measured by rFID, SSIM, PSNR, and LPIPS, but also converge faster compared to those using fewer CLIP latent tokens.
          In Figure 3, we show qualitative examples of images generated with different numbers of CLIP latent tokens.
          </p>
        </div>

        <br><br>
        <img src="./static/images/figure4.png" alt="Teaser" width="95%">
        <div class="content has-text-justified">
          <b>Figure 4. Image reconstruction comparison with state-of-the-art methods</b>
        </div>
        <div class="content has-text-justified">
          <p>
          In Figure 4, we qualitatively compare the image reconstruction quality of our latent ControlNet with various unified models, including SEED, EMU, EMU2, GPT-4o, and MetaQuery.
          Specifically, we first encode the images using the visual encoder in Qwen2.5-VL, then use the latent ControlNet to reconstruct the images based solely on this visual information, without providing any text prompt as additional guidance.
          Trained exclusively on the ImageNet dataset for 3 epochs without exposure to any other open-world images, the reconstructions from Bifrost-1 latent ControlNet achieve quality that is competitive with or superior to strong baselines such as GPT-4o and MetaQuery, demonstrating the efficiency and robustness of our Latent ControlNet design.
          </p>
        </div>


        <br><br>


        <h3 class="title is-4">Visualization Examples</h3>

        <div class="content has-text-justified">
          We provide image generation visualization examples of our Bifrost-1 and other baselines in the following Figure 5 and Figure 6.
        </div>

        <img src="./static/images/figure5.png" alt="Teaser" width="90%">
        <div class="content has-text-justified">
          <b>Figure 5. Qualitative comparision with state-of-the-art methods on MJHQ30k dataset</b>
        </div>

        <br><br>

        <img src="./static/images/figure6.png" alt="Teaser" width="90%">
        <div class="content has-text-justified">
          <b>Figure 6. Bifrost-1 visualization examples on MJHQ30k dataset</b>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop content">
    <h3 class="title">Limitations</h3>
    <p style="font-size: 0.8em;">
    Note that Bifrost-1 is designed as a bridging method that connects existing MLLMs with diffusion-based image generation models.
    As such, its performance, output quality, and potential visual artifacts are inherently influenced by the capabilities and limitations of the underlying backbone models it relies on.
    For instance, if the diffusion model used as the visual backbone struggles with generating complex, rare, or previously unseen scenes and objects, then Bifrost-1, which builds upon this foundation, may also exhibit suboptimal image generation results.
    This dependency highlights the importance of selecting strong and well-generalized base models when applying Bifrost-1 to real-world or open-domain generation tasks.
    </p>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>

    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          The webpage was adapted from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
